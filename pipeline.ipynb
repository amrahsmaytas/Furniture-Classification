{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from _preprocess import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.placeholder(tf.float32, (None, 227, 227, 3))\n",
    "labels = tf.placeholder(tf.int64, None)\n",
    "classes = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 2000\n",
      "Finished processing 4000\n",
      "Finished processing 6000\n",
      "Finished processing 8000\n",
      "Finished processing 10000\n",
      "Finished processing 12000\n",
      "Finished processing 14000\n",
      "Finished processing 16000\n",
      "Finished processing 18000\n",
      "Finished processing 20000\n",
      "Finished processing 22000\n",
      "Finished processing 24000\n",
      "Finished processing 26000\n",
      "Finished processing 28000\n",
      "Finished processing 30000\n",
      "Finished processing 32000\n",
      "Finished processing 34000\n",
      "Finished processing 36000\n",
      "Finished processing 38000\n",
      "Finished processing 40000\n",
      "Finished processing 42000\n",
      "Finished processing 44000\n",
      "Finished processing 46000\n",
      "Finished processing 48000\n",
      "Finished processing 50000\n",
      "Finished processing 52000\n",
      "Finished processing 54000\n",
      "Finished processing 56000\n",
      "Finished processing 58000\n",
      "Finished processing 60000\n",
      "Finished processing 62000\n",
      "Finished processing 64000\n",
      "Finished processing 66000\n",
      "Finished processing 68000\n",
      "Finished processing 70000\n",
      "Finished processing 72000\n",
      "Finished processing 74000\n",
      "Finished processing 76000\n",
      "Finished processing 78000\n",
      "Finished processing 80000\n",
      "Finished processing 84000\n",
      "Finished processing 86000\n",
      "Finished processing 88000\n",
      "Finished processing 90000\n",
      "Finished processing 92000\n",
      "Finished processing 94000\n",
      "Finished processing 96000\n",
      "Finished processing 98000\n",
      "Finished processing 100000\n",
      "Finished processing 106000\n",
      "Finished processing 108000\n",
      "Finished processing 110000\n",
      "Finished processing 112000\n",
      "Finished processing 114000\n",
      "Finished processing 116000\n",
      "Finished processing 118000\n",
      "Finished processing 120000\n",
      "Finished processing 122000\n",
      "Finished processing 124000\n",
      "Finished processing 126000\n",
      "Finished processing 128000\n",
      "Finished processing 130000\n",
      "Finished processing 132000\n",
      "Finished processing 134000\n",
      "Finished processing 136000\n",
      "Finished processing 138000\n",
      "Finished processing 140000\n",
      "Finished processing 142000\n",
      "Finished processing 144000\n",
      "Finished processing 146000\n",
      "Finished processing 148000\n",
      "Finished processing 150000\n",
      "Finished processing 152000\n",
      "Finished processing 154000\n",
      "Finished processing 156000\n",
      "Finished processing 158000\n",
      "Finished processing 160000\n",
      "Finished processing 162000\n",
      "Finished processing 164000\n",
      "Finished processing 166000\n",
      "Finished processing 168000\n",
      "Finished processing 170000\n",
      "Finished processing 172000\n",
      "Finished processing 174000\n",
      "Finished processing 176000\n",
      "Finished processing 178000\n",
      "Finished processing 180000\n",
      "Finished processing 182000\n",
      "Finished processing 184000\n",
      "Finished processing 186000\n",
      "Finished processing 188000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6fbbd2cfb9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.5/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = preprocess_train(False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:160000]\n",
    "X_val = X[160000:]\n",
    "y_train = y[:160000]\n",
    "y_val = y[160000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train - 1\n",
    "y_val = y_val - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc7 = AlexNet(features, feature_extract=True)\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "shape = (fc7.get_shape().as_list()[-1], classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "preds = tf.argmax(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "def eval_on_data(X, y, sess):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        end = offset + batch_size\n",
    "        X_batch = X[offset:end]\n",
    "        y_batch = y[offset:end]\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={features: X_batch, labels: y_batch})\n",
    "        total_loss += (loss * X_batch.shape[0])\n",
    "        total_acc += (acc * X_batch.shape[0])\n",
    "\n",
    "    return total_loss/X.shape[0], total_acc/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Time: 547.099 seconds\n",
      "Validation Loss = 1.39072235032\n",
      "Validation Accuracy = 0.650908443216\n",
      "\n",
      "Epoch 2\n",
      "Time: 392.591 seconds\n",
      "Validation Loss = 1.42471905827\n",
      "Validation Accuracy = 0.657784111189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # training\n",
    "#         X_train, y_train = shuffle(X_train, y_train)\n",
    "        t0 = time.time()\n",
    "        for offset in range(0, X_train.shape[0], batch_size):\n",
    "            end = offset + batch_size\n",
    "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "\n",
    "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
    "        print(\"Epoch\", i+1)\n",
    "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
    "        print(\"Validation Loss =\", val_loss)\n",
    "        print(\"Validation Accuracy =\", val_acc)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
